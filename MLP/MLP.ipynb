{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMvlvtByLdU1",
        "colab_type": "text"
      },
      "source": [
        "# **Checking tensorflow version**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsHXPkOLDAkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb2X_87sDIx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## only run this if the tf version is less than 2.0.0\n",
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmcG7B_UHhS1",
        "colab_type": "text"
      },
      "source": [
        "# **Tensorflow Low Level API for MNIST dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbixoE-VfVNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Install dependencies\n",
        "\n",
        "import datetime, os\n",
        "import numpy as np \n",
        "import tensorflow as tf \n",
        "from tqdm import trange\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snLG0ZvJgTx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Imports MNIST dataset\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWkodtPQqvyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Write a function to reprocess the data for input into the MLP. It must:\n",
        "## 1) Reshape data into 2D matrix(whereby each example is a row, and each column represents a pixel value)\n",
        "## 2) Convert the labels into one-hot encoding(hint: use scikitlearn LabelBinarizer)\n",
        "\n",
        "def mnist_preprocess(X_train, X_test, y_train, y_test, train_size=0.7):\n",
        "    ## splits dataset into train/val/test\n",
        "    training_set_size = round((X_train.shape[0]) * train_size)\n",
        "    X_train = X_train.reshape(X_train.shape[0], (X_train.shape[1] * X_train.shape[2]))\n",
        "    X_train, X_val = X_train[:training_set_size], X_train[training_set_size:]\n",
        "    X_test = X_test.reshape(X_test.shape[0], (X_test.shape[1] * X_test.shape[2]))\n",
        "    temp1 = np.zeros((y_train.shape[0], y_train.max() + 1))\n",
        "    for i, j in enumerate(y_train):\n",
        "        temp1[i][j] = 1\n",
        "    y_train, y_val = temp1[:training_set_size], temp1[training_set_size:]\n",
        "    temp2 = np.zeros((y_test.shape[0], y_test.max() + 1))\n",
        "    for i, j in enumerate(y_test):\n",
        "        temp2[i][j] = 1\n",
        "    y_test = temp2\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
        "\n",
        "(X_train, y_train), (X_val, y_val), (X_test, y_test) = mnist_preprocess(X_train, X_test, y_train, y_test, train_size=0.7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52JEz7NckPTk",
        "colab_type": "text"
      },
      "source": [
        "Feedforward propagation. Similar to linear and logistic regression, neural networks require a feedforward portion which calculates the output in a series of operations and transformations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkbecFZAgSA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Define variables and placeholders for a 4 layer MLP(1st layer - 784 units, 2nd layer - 500 units, , 3rd layer - 100 units, last layer - 10 units)\n",
        "\n",
        "n_input = 784\n",
        "n_hidden_1 = 500\n",
        "n_hidden_2 = 100\n",
        "output = 10\n",
        "batch_size = 2000\n",
        "training_epochs = 8\n",
        "learning_rate = 0.0005"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPFEfv14J6pE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Define placeholders for inputs and outputs\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, n_input])\n",
        "y = tf.placeholder(tf.float32, [None, output])\n",
        "\n",
        "## Define the additional weights and biases. W1 and b1 have been done for you.\n",
        "\n",
        "W1 = tf.Variable(tf.random_normal([n_input, n_hidden_1]))\n",
        "b1 = tf.Variable(tf.random_normal([n_hidden_1]))\n",
        "W2 = tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]))\n",
        "b2 = tf.Variable(tf.random_normal([n_hidden_2]))\n",
        "W3 = tf.Variable(tf.random_normal([n_hidden_2, output]))\n",
        "b3 = tf.Variable(tf.random_normal([output]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxZdKuUTgQQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Construct the hypothesis for the MLP. The first 2 steps have been done for you. \n",
        "y_preact_1 = tf.add(tf.matmul(x, W1), b1)\n",
        "y_act_1 = tf.nn.relu(y_preact_1)\n",
        "y_preact_2 = tf.add(tf.matmul(y_act_1, W2), b2)\n",
        "y_act_2 = tf.nn.relu(y_preact_2)\n",
        "y_preact_3 = tf.add(tf.matmul(y_act_2, W3), b3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-aodI67VwZo",
        "colab_type": "text"
      },
      "source": [
        "The loss we commonly use in classification is cross-entropy. Cross-entropy is a concept from information theory:\n",
        "\n",
        "ùêª(ùë¶)=‚àí‚àëùë¶‚Ä≤log(ùë¶)\n",
        "\n",
        "Cross-entropy not only captures how correct (max probability corresponds to the right answer) the model's answers are, it also accounts for how confident (high confidence in correct answers) they are. This encourages the model to produce very high probabilities for correct answers while driving down the probabilities for the wrong answers, instead of merely be satisfied with it being the argmax.\n",
        "\n",
        "Thereafter, (feed)back propagation takes over. This is unique to neural networks and not seen in linear or logistic regression. This is because the \"deep\" architecture of neural networks makes finding out the partial derivative of the cost function difficult, and hence the need for back propagation. Intuitively, it can be seen as finding out the contribution of each node in each layer that contributes to the total cost, and thereby adjusting the weights accordingly. For a more detailed explanation and the math behind it: https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSCjHMPgVsug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Define cost function \n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "    labels=y,\n",
        "    logits=y_preact_3))\n",
        "\n",
        "## Optimizer\n",
        "\n",
        "train_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1g0pDJcpeFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Run model training\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  avg_training_cost = []\n",
        "  avg_validation_cost = []\n",
        "  epoch = []\n",
        "  for j in trange(training_epochs):\n",
        "    epoch.append(j)\n",
        "    average_training_cost = 0\n",
        "    average_validation_cost = 0\n",
        "    total_num_of_training_batches = int(X_train.shape[0] / batch_size)\n",
        "    total_num_of_validation_batches = int(X_val.shape[0] / batch_size)\n",
        "    for i in range(total_num_of_training_batches - 1):\n",
        "      batch_x = X_train[(batch_size*i) : (batch_size*(i+1))]\n",
        "      batch_y = y_train[(batch_size*i) : (batch_size*(i+1))]\n",
        "      _, c = sess.run([train_optimizer, cost], feed_dict={x: batch_x, y: batch_y}) #feed_dict is for all the placeholders defined above\n",
        "      # print(\"Batch {} of epoch {} done!\".format(i, j))\n",
        "      average_training_cost += c / total_num_of_training_batches\n",
        "    avg_training_cost.append(average_training_cost)\n",
        "    for i in range(total_num_of_validation_batches - 1):\n",
        "      batch_x = X_val[(batch_size*i) : (batch_size*(i+1))]\n",
        "      batch_y = y_val[(batch_size*i) : (batch_size*(i+1))]\n",
        "      c = sess.run(cost, feed_dict={x:X_val, y:y_val})\n",
        "      average_validation_cost += c / total_num_of_validation_batches\n",
        "    avg_validation_cost.append(average_validation_cost)\n",
        "    print(\"Epoch {} done!\".format(j))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzFTpa0xouLg",
        "colab_type": "text"
      },
      "source": [
        "Debugging and evaluation of model performance is the most important part of machine learning model construction, and oftentimes takes up the most amount of time!!! We use the cost curves to help inform us about the model performance, and infer about the tweaks (especially in the hyperparameters) that we need to make to improve results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omvajk-wdZ1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Plot cost curves\n",
        "# print(epoch, avg_training_cost, avg_validation_cost)\n",
        "plt.plot(epoch, avg_training_cost)\n",
        "plt.plot(epoch, avg_validation_cost)\n",
        "plt.legend(['train', 'valid'])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"cost curves\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EAtRxXIexuN",
        "colab_type": "text"
      },
      "source": [
        "# **KERAS WITH TENSORBOARD**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ2ocQsNerS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Builds model with hyperparameters defined\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(n_input, input_dim=X_train.shape[1], activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(n_hidden_1, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(n_hidden_2, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(output, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate), \n",
        "             loss=tf.keras.losses.categorical_crossentropy,\n",
        "             metrics=[tf.keras.metrics.categorical_accuracy])\n",
        "\n",
        "## Model training \n",
        "\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "callbacks = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "training = model.fit(X_train, y_train, callbacks=[callbacks], epochs=training_epochs, batch_size=batch_size, validation_data=(X_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8ZaC7_LnEk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Plotting cost curves (preferred)\n",
        "\n",
        "plt.plot(training.history['loss'])\n",
        "plt.plot(training.history['val_loss'])\n",
        "plt.title('cost curves')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCFs_eI0nfA0",
        "colab_type": "text"
      },
      "source": [
        "Or, you can use TensorBoard if you're confident..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me6rBUGKnCYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip uninstall -q -y tensorboard\n",
        "!pip uninstall -q -y tensorflow\n",
        "# Install nightly TensorFlow with nightly TensorBoard.\n",
        "!pip install --ignore-installed tf-nightly\n",
        "!rm -rf ./logs/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_81QwKh5W7bX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Loads tensorboard utilities\n",
        "\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}