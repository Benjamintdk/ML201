{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Part_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVRN1uL20HMl",
        "colab_type": "text"
      },
      "source": [
        "# Checking tensorflow version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKaooaugMpwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BwefeAQzsVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## only run this if the tf version is less than 2.1.0\n",
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W47oSqUF0prn",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Recurrent Neural Networks with Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zF5YXme0toV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## imports necessary dependencies and utilities\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from matplotlib import pyplot as plt \n",
        "from tensorflow.keras import Sequential, layers, optimizers, losses, metrics\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOPAo9hN3LFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## splits the dataset up into training and test sets\n",
        "## vocab size refers to the number of unique words to be kept in the reviews \n",
        "\n",
        "vocab_size = 5000\n",
        "max_length = 500\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
        "                                                      num_words=vocab_size,\n",
        "                                                      seed=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F7J-tqcEbaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## check the format of the data (complete the code)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTK3AY6qEaJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## return the list of words of the first review (complete the code)\n",
        "\n",
        "word2id = imdb.get_word_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G-GQDjcFQmv",
        "colab_type": "text"
      },
      "source": [
        "Hmm what else can we check? We just learnt that checking the length of sequences are important in sequence models. Thereafter, we can decide on a standardised sequence length for each review. Reviews longer than that will be truncated, while reviews shorter than that will be padded. If you are interested in how post or pre padding will affect results, you can refer to this paper: https://arxiv.org/abs/1903.07288#:~:text=Since%20LSTMs%20and%20CNNs%20take,comes%20to%20performance%20and%20accuracies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb0zIqMJFMVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## prints length of longest review \n",
        "\n",
        "print('Maximum review length: {}'.format(len(max((X_train + X_test))), key=len))\n",
        "\n",
        "# ## prints length of shortest review \n",
        "\n",
        "print('Minimum review length: {}'.format(len(min((X_train + X_test))), key=len))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETrv1lQMwK9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## max length of sequence is established and the reviews and correspondingly padded\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_length, padding='pre', truncating='post')\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_length, padding='pre', truncating='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rmnX6fdIYt6",
        "colab_type": "text"
      },
      "source": [
        "Now that the dataset has been preprocessed(if you want to learn more about text preprocessing: https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79), we can start instantiating a model and training it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsQDLiBRNI1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## define some variables first\n",
        "\n",
        "word_vector_size = 128\n",
        "num_epochs = 5\n",
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otRV1SEFIX0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## model instantiation \n",
        "\n",
        "model = Sequential()\n",
        "## the first embedding layer is meant to help convert the word ids to vectors of length vector_size\n",
        "model.add(layers.Embedding(vocab_size, word_vector_size, input_length=max_length))\n",
        "## the second layer is a simple RNN layer with the output vector from this layer being (batch_size, num_of units)\n",
        "model.add(layers.SimpleRNN(128, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2))\n",
        "## the last layer is a binary classification layer\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "## model compilation \n",
        "\n",
        "model.compile(loss='binary_crossentropy', \n",
        "             optimizer=optimizers.Adam(0.001), \n",
        "             metrics=['accuracy'])\n",
        "\n",
        "## run model training \n",
        "\n",
        "trained = model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_split=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axJPTy_yhoky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## visualise the model training by plotting loss curves\n",
        "\n",
        "  \n",
        "acc = trained.history['accuracy']\n",
        "val_acc = trained.history['val_accuracy']\n",
        "\n",
        "loss = trained.history['loss']\n",
        "val_loss = trained.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7yTrRCwZDz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## evaluate the model on the test set \n",
        "\n",
        "scores = model.evaluate(X_test, y_test)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}